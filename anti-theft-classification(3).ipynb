{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12673154,"sourceType":"datasetVersion","datasetId":8008745}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2 \nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm \nfrom torchvision import models\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T12:58:23.502785Z","iopub.execute_input":"2025-08-07T12:58:23.503428Z","iopub.status.idle":"2025-08-07T12:58:32.081772Z","shell.execute_reply.started":"2025-08-07T12:58:23.503403Z","shell.execute_reply":"2025-08-07T12:58:32.081003Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/anti-theft-dataset/Shop DataSet\"\nCLASSES = [\"non shop lifters\", \"shop lifters\"]\nLABEL_MAP = {label: i for i, label in enumerate(CLASSES)}\n\n\nvideo_data = []\nfor class_name in CLASSES:\n    class_dir = os.path.join(DATASET_PATH, class_name)\n    if not os.path.isdir(class_dir):\n        print(f\"Warning: Directory not found at {class_dir}\")\n        continue\n    for video_file in os.listdir(class_dir):\n        video_path = os.path.join(class_dir, video_file)\n        video_data.append({'path': video_path, 'label': LABEL_MAP[class_name]})\n\n\ndf = pd.DataFrame(video_data)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n\ntrain_df, test_df = train_test_split(\n    df, test_size=0.20, random_state=42, stratify=df['label']\n)\n\nprint(f\"Total videos: {len(df)}\")\nprint(f\"Training set size: {len(train_df)}\")\nprint(f\"Test set size: {len(test_df)}\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T12:58:32.083015Z","iopub.execute_input":"2025-08-07T12:58:32.083328Z","iopub.status.idle":"2025-08-07T12:58:32.143871Z","shell.execute_reply.started":"2025-08-07T12:58:32.083311Z","shell.execute_reply":"2025-08-07T12:58:32.143302Z"}},"outputs":[{"name":"stdout","text":"Total videos: 855\nTraining set size: 684\nTest set size: 171\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                path  label\n0  /kaggle/input/anti-theft-dataset/Shop DataSet/...      0\n1  /kaggle/input/anti-theft-dataset/Shop DataSet/...      0\n2  /kaggle/input/anti-theft-dataset/Shop DataSet/...      0\n3  /kaggle/input/anti-theft-dataset/Shop DataSet/...      0\n4  /kaggle/input/anti-theft-dataset/Shop DataSet/...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/anti-theft-dataset/Shop DataSet/...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/anti-theft-dataset/Shop DataSet/...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/anti-theft-dataset/Shop DataSet/...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/anti-theft-dataset/Shop DataSet/...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/anti-theft-dataset/Shop DataSet/...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"IMG_SIZE = 128\nMAX_FRAMES = 20 \n\ntrain_transform = transforms.Compose([\n    transforms.ToTensor(),\n    # Augmentations are applied here\n    transforms.RandomHorizontalFlip(p=0.5), # Randomly flip the video horizontally\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    # Resize and Normalize are applied last\n    transforms.Resize((IMG_SIZE, IMG_SIZE), antialias=True),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n# For validation/testing, we only resize and normalize, no augmentation\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((IMG_SIZE, IMG_SIZE), antialias=True),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndef extract_frames(video_path, transform):\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_indices = np.linspace(0, total_frames - 1, MAX_FRAMES, dtype=int)\n\n    for i in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(transform(frame))\n        else:\n            frames.append(torch.zeros((3, IMG_SIZE, IMG_SIZE)))\n\n    cap.release()\n    return torch.stack(frames)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T12:58:32.144464Z","iopub.execute_input":"2025-08-07T12:58:32.144647Z","iopub.status.idle":"2025-08-07T12:58:32.151564Z","shell.execute_reply.started":"2025-08-07T12:58:32.144633Z","shell.execute_reply":"2025-08-07T12:58:32.150764Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    # Accept a transform\n    def __init__(self, df, transform):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        video_path = self.df.iloc[idx]['path']\n        label = self.df.iloc[idx]['label']\n        \n        # Use the passed transform\n        frames = extract_frames(video_path, self.transform) # Shape: (Time, Channels, H, W)\n        \n        # --- THIS IS THE FIX ---\n        # Permute the dimensions to (Channels, Time, H, W) for the 3D CNN\n        frames = frames.permute(1, 0, 2, 3) \n        \n        return frames, torch.tensor(label, dtype=torch.float32)\n\n# Create Datasets with the correct transforms\ntrain_dataset = VideoDataset(train_df, transform=train_transform)\ntest_dataset = VideoDataset(test_df, transform=test_transform)\n\n# Create DataLoader objects (with num_workers for performance)\nBATCH_SIZE = 8\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T12:58:32.153084Z","iopub.execute_input":"2025-08-07T12:58:32.153273Z","iopub.status.idle":"2025-08-07T12:58:32.170790Z","shell.execute_reply.started":"2025-08-07T12:58:32.153258Z","shell.execute_reply":"2025-08-07T12:58:32.170184Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class CNN3D(nn.Module):\n    def __init__(self, num_classes=1):\n        super(CNN3D, self).__init__()\n        \n        self.conv_layer1 = nn.Sequential(\n            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), padding=1),\n            nn.ReLU(),\n            nn.MaxPool3d((1, 2, 2)) # Reduce H, W but not Time\n        )\n        self.conv_layer2 = nn.Sequential(\n            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1),\n            nn.ReLU(),\n            nn.MaxPool3d((2, 2, 2)) # Reduce T, H, W\n        )\n        self.conv_layer3 = nn.Sequential(\n            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1),\n            nn.ReLU(),\n            nn.MaxPool3d((2, 2, 2)) # Reduce T, H, W\n        )\n        \n        # Adaptive pooling to handle any remaining dimensions\n        self.adaptive_pool = nn.AdaptiveAvgPool3d(1)\n        \n        self.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, num_classes)\n        )\n\n    def forward(self, x):\n        # x shape: (batch_size, C, time_steps, H, W)\n        out = self.conv_layer1(x)\n        out = self.conv_layer2(out)\n        out = self.conv_layer3(out)\n        \n        out = self.adaptive_pool(out)\n        \n        # Flatten the features for the fully connected layer\n        out = out.view(out.size(0), -1) \n        \n        out = self.fc(out)\n        return out\n\n# Instantiate the model and move it to the device\nmodel = CNN3D().to(device)\nprint(\"Model created: 3D CNN (From Scratch)\")\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T12:58:32.171405Z","iopub.execute_input":"2025-08-07T12:58:32.171573Z","iopub.status.idle":"2025-08-07T12:58:32.357555Z","shell.execute_reply.started":"2025-08-07T12:58:32.171559Z","shell.execute_reply":"2025-08-07T12:58:32.356906Z"}},"outputs":[{"name":"stdout","text":"Model created: 3D CNN (From Scratch)\nCNN3D(\n  (conv_layer1): Sequential(\n    (0): Conv3d(3, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n    (1): ReLU()\n    (2): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_layer2): Sequential(\n    (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n    (1): ReLU()\n    (2): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_layer3): Sequential(\n    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n    (1): ReLU()\n    (2): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (adaptive_pool): AdaptiveAvgPool3d(output_size=1)\n  (fc): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=64, out_features=32, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=32, out_features=1, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss() \noptimizer = optim.Adam(model.parameters(), lr=0.0001) \nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2)\n\ndef train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train() \n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n    \n    for inputs, labels in tqdm(loader, desc=\"Training\"):\n        inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n        preds = torch.sigmoid(outputs) > 0.5\n        correct_predictions += (preds == labels).sum().item()\n        total_samples += labels.size(0)\n        \n    epoch_loss = running_loss / total_samples\n    epoch_acc = correct_predictions / total_samples\n    return epoch_loss, epoch_acc\n\ndef validate_one_epoch(model, loader, criterion, device):\n    model.eval() \n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n    \n    with torch.no_grad(): \n        for inputs, labels in tqdm(loader, desc=\"Validation\"):\n            inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item() * inputs.size(0)\n            preds = torch.sigmoid(outputs) > 0.5\n            correct_predictions += (preds == labels).sum().item()\n            total_samples += labels.size(0)\n            \n    epoch_loss = running_loss / total_samples\n    epoch_acc = correct_predictions / total_samples\n    return epoch_loss, epoch_acc\n\n\nNUM_EPOCHS = 20\nhistory = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\nbest_val_acc = 0.0\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_acc = validate_one_epoch(model, test_loader, criterion, device)\n    \n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    \n    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}   | Val Acc: {val_acc:.4f}\")\n    \n    scheduler.step(val_loss)\n    \n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(\"--- Best model saved! ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T12:58:32.358300Z","iopub.execute_input":"2025-08-07T12:58:32.358574Z","execution_failed":"2025-08-07T15:05:38.487Z"}},"outputs":[{"name":"stdout","text":"\n--- Epoch 1/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d909f0831f694e25a6775ad6f5c053ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d25fa11d40f439bad9bfac7442ef58b"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6859 | Train Acc: 0.5556\nVal Loss: 0.6708   | Val Acc: 0.6199\n--- Best model saved! ---\n\n--- Epoch 2/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eb632679feb492f92dd927bcaf357b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9759a98686c4966b2d80e72b2b6fdd5"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6712 | Train Acc: 0.6213\nVal Loss: 0.6710   | Val Acc: 0.6199\n\n--- Epoch 3/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"765224df506a4ecca628c6d7d872619b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b414417143442e5a387f31d0174df77"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6774 | Train Acc: 0.6199\nVal Loss: 0.6691   | Val Acc: 0.6199\n\n--- Epoch 4/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a777642f36fb4b18b0ca456e2510a680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee109b8c5264ffe82c994a5a5ab0a82"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6707 | Train Acc: 0.6213\nVal Loss: 0.6649   | Val Acc: 0.6199\n\n--- Epoch 5/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb868b87cc2844a2a6eaf8b8e9c18d4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"820ed9619451493c82748ac71439c648"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6694 | Train Acc: 0.6213\nVal Loss: 0.6644   | Val Acc: 0.6199\n\n--- Epoch 6/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b3855ae962b4741b7a7ba9fe86ae8e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83621fb97087449db97800728a83c852"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6683 | Train Acc: 0.6213\nVal Loss: 0.6674   | Val Acc: 0.6199\n\n--- Epoch 7/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"096e1ca747ed421b846b16f7e162ace0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b669c80c796d41039b526bff253df1fc"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6658 | Train Acc: 0.6213\nVal Loss: 0.6681   | Val Acc: 0.6199\n\n--- Epoch 8/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a19d2279aacd4dae8272feadaf10514b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1001c4da66946b9b9d88575c82f0fc8"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6702 | Train Acc: 0.6213\nVal Loss: 0.6651   | Val Acc: 0.6199\n\n--- Epoch 9/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce1ab57381004d3eb144ce3c973ded53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb31e528a654420bb30d38d1b31dfbc"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6646 | Train Acc: 0.6199\nVal Loss: 0.6661   | Val Acc: 0.6199\n\n--- Epoch 10/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee0ae9f70f64101a5edaaa03f9441c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d15b52ef9b347e88e65a1ac017e8284"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6651 | Train Acc: 0.6213\nVal Loss: 0.6635   | Val Acc: 0.6199\n\n--- Epoch 11/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ee4d01f0de043d881afced9dc80c81d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64ce5433cd824f21bfd1fb93e3ef9b16"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6695 | Train Acc: 0.6213\nVal Loss: 0.6670   | Val Acc: 0.6199\n\n--- Epoch 12/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6574012597514dcca8e9ea098304048d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceac364ecb8a4c91abcfbd5a81af79fe"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6648 | Train Acc: 0.6228\nVal Loss: 0.6632   | Val Acc: 0.6199\n\n--- Epoch 13/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a43e31398bde4ee6b3138ae14bf9f47f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feb8a1dbf0964b938e8761de7c73c2f1"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6687 | Train Acc: 0.6213\nVal Loss: 0.6649   | Val Acc: 0.6199\n\n--- Epoch 14/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e6c57fae62c4ede983d676d11ad613a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfe954c3fcf84cb5a61ee6da3b8eb952"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.6667 | Train Acc: 0.6213\nVal Loss: 0.6656   | Val Acc: 0.6199\n\n--- Epoch 15/20 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b333e3dd8d482791c8538b013b68aa"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}